import { AnalyzeInput, ReportSections, CoverageMeta, ContextPack, RawItem } from '@/lib/types';
import { DEFAULT_SUBREDDITS } from '@/lib/subreddits';
import { PROGRESS_WEIGHTS } from '@/components/SmoothProgress';

export interface AnalysisResult {
  report: ReportSections;
  coverage: CoverageMeta;
}

export interface ProgressCallback {
  (step: string, substatus?: string): void;
}

export interface LogCallback {
  (log: string): void;
}

export interface ProgressUpdateCallback {
  (progress: { percentage: number; taskIndex: number }): void;
}

// Enhanced streaming analysis function
export async function runAnalysisWithStreaming(
  input: AnalyzeInput,
  onLog: LogCallback,
  onProgress: ProgressUpdateCallback
): Promise<AnalysisResult> {
  try {
    // Step 1: Build context (10%)
    onProgress({ percentage: PROGRESS_WEIGHTS.CONTEXT, taskIndex: 0 });
    onLog(`[Context] Analyzing ${input.me.name} to understand product category and features...`);
    
    const meRes = await fetch('/api/mine/context', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(input.me),
    });
    
    if (!meRes.ok) {
      const error = await meRes.json();
      throw new Error(error.error || 'Failed to analyze your product');
    }
    
    const meContext: ContextPack = await meRes.json();
    onLog(`[Context] âœ“ Generated context keywords and category for ${input.me.name}`);
    
    // Step 2: Mine MY PRODUCT (30%)
    onProgress({ percentage: PROGRESS_WEIGHTS.CONTEXT + PROGRESS_WEIGHTS.MINE_ME, taskIndex: 1 });
    onLog(`[Crawler] ðŸ” Searching ${input.subreddits.length} subreddits for discussions about ${input.me.name}...`);
    
    const mineMe = await streamCall('/api/fetch/reddit', {
      target: 'me',
      ...input,
      subreddits: input.subreddits.length > 0 ? input.subreddits : DEFAULT_SUBREDDITS,
      meContext,
    }, onLog) as RawItem[];
    
    onLog(`[Crawler] âœ“ Found ${mineMe.length} discussions about ${input.me.name}`);
    
    // Step 3: Mine COMPETITORS (55%)
    onProgress({ percentage: PROGRESS_WEIGHTS.CONTEXT + PROGRESS_WEIGHTS.MINE_ME + PROGRESS_WEIGHTS.MINE_COMPETITORS, taskIndex: 2 });
    onLog(`[Crawler] ðŸ” Searching ${input.subreddits.length} subreddits for discussions about competitors: ${input.competitors.map(c => c.name).join(', ')}...`);
    
    const mineCompetitors = await streamCall('/api/fetch/reddit', {
      target: 'competitors',
      ...input,
      subreddits: input.subreddits.length > 0 ? input.subreddits : DEFAULT_SUBREDDITS,
      meContext,
    }, onLog) as RawItem[];
    
    onLog(`[Crawler] âœ“ Found ${mineCompetitors.length} discussions about competitors`);
    
    // Step 4: Filter (65%)
    onProgress({ percentage: PROGRESS_WEIGHTS.CONTEXT + PROGRESS_WEIGHTS.MINE_ME + PROGRESS_WEIGHTS.MINE_COMPETITORS + PROGRESS_WEIGHTS.FILTER, taskIndex: 3 });
    onLog(`[Filter] ðŸŽ¯ Filtering ${mineMe.length + mineCompetitors.length} posts/comments for UX and feedback relevance...`);
    
    // Step 5: Classify (80%)
    onProgress({ percentage: PROGRESS_WEIGHTS.CONTEXT + PROGRESS_WEIGHTS.MINE_ME + PROGRESS_WEIGHTS.MINE_COMPETITORS + PROGRESS_WEIGHTS.FILTER + PROGRESS_WEIGHTS.CLASSIFY, taskIndex: 4 });
    onLog(`[Classifier] ðŸ“Š Classifying discussions into aspects (performance, pricing, features, etc.)...`);
    
    // Step 6: Compose (95%)
    onProgress({ percentage: PROGRESS_WEIGHTS.CONTEXT + PROGRESS_WEIGHTS.MINE_ME + PROGRESS_WEIGHTS.MINE_COMPETITORS + PROGRESS_WEIGHTS.FILTER + PROGRESS_WEIGHTS.CLASSIFY + PROGRESS_WEIGHTS.COMPOSE, taskIndex: 5 });
    onLog(`[Writer] ðŸ“ Generating comprehensive competitive intelligence report with insights and recommendations...`);
    
    const allItems = [...mineMe, ...mineCompetitors];

    // Try server analysis first; if it fails, synthesize a local report so we never crash
    let result: AnalysisResult | null = null;
    try {
      result = await streamCall('/api/analyze', {
        items: allItems,
        input,
        meContext,
      }, onLog) as AnalysisResult;
    } catch (serverErr) {
      onLog(`[Writer] âš ï¸ Server analysis failed. Falling back to local heuristic report.`);
      result = buildLocalFallbackReport(allItems, input, meContext, onLog);
    }
    
    // Step 7: CSV (100%)
    onProgress({ percentage: 100, taskIndex: 6 });
    onLog(`[CSV] ðŸ“‹ Exporting detailed data to CSV with ${allItems.length} analyzed discussions...`);
    onLog(`[System] ðŸŽ‰ Analysis complete! Generated report with insights from ${allItems.length} Reddit discussions.`);
    
    return result;
  } catch (error) {
    onLog(`[System] âŒ Error: ${error instanceof Error ? error.message : 'Unknown error'}`);
    throw error;
  }
}

// ------------------
// Local fallback report generator (client-side) to guarantee output
// ------------------
function buildLocalFallbackReport(
  items: RawItem[],
  input: AnalyzeInput,
  _meContext: ContextPack,
  onLog: LogCallback
): AnalysisResult {
  const coverage: CoverageMeta = {
    days: input.days,
    totalThreads: items.filter(i => i.type === 'post').length,
    totalComments: items.filter(i => i.type === 'comment').length,
    totalItemsUsed: items.length,
    subredditsUsed: new Set(items.map(i => i.subreddit)).size,
  };

  const md = generateHeuristicMarkdown(items, input, coverage);

  const report: ReportSections = {
    header: '# **Competitive Intelligence Report**\n\n',
    launches: '',
    loving: '',
    notLoving: '',
    strategicRead: '',
    takeaways: '',
    appendixCsv: buildCsv(items),
    raw: md,
  };

  onLog(`[Writer] âœ… Generated local fallback report with ${items.length} items across ${coverage.subredditsUsed} subreddits.`);
  return { report, coverage };
}

function generateHeuristicMarkdown(items: RawItem[], input: AnalyzeInput, coverage: CoverageMeta): string {
  const byCompetitor: Record<string, RawItem[]> = {};
  for (const it of items.slice(0, 250)) {
    const key = it.matchedCompetitor || 'Unknown';
    (byCompetitor[key] ||= []).push(it);
  }

  const posRx = /(love|awesome|great|fast|amazing|improve|like|good|easy|works|best)/i;
  const negRx = /(bug|crash|slow|issue|problem|expensive|pricey|hate|bad|hard|doesn\'t|broken|error|downtime)/i;
  const updRx = /(launch|release|update|version|v\d|beta|ga|announc)/i;

  const sectionForProduct = (name: string, arr: RawItem[]) => {
    const updates = arr.filter(a => updRx.test(a.title_or_text)).slice(0, 3);
    const loves = arr.filter(a => posRx.test(a.title_or_text)).slice(0, 4);
    const notLoves = arr.filter(a => negRx.test(a.title_or_text)).slice(0, 4);

    const list = (xs: RawItem[]) => xs.map(x => `â€¢ ${truncate(x.title_or_text, 140)}\n  [ref](${x.thread_url})`).join('\n');

    return [
      `### **${name}**`,
      ``,
      `#### **ðŸš€ New Updates**`,
      updates.length ? list(updates) : 'â€¢ No major launches mentioned in this window.',
      ``,
      `#### **ðŸ’š What Users Love**`,
      loves.length ? list(loves) : 'â€¢ Positive mentions exist but were not strongly classified in this window.',
      ``,
      `#### **âš ï¸ What Users Dislike**`,
      notLoves.length ? list(notLoves) : 'â€¢ Few explicit complaints detected in this window.',
      ``,
      `---`,
    ].join('\n');
  };

  const competitorsMd = [input.me.name, ...input.competitors.map(c => c.name)]
    .filter(Boolean)
    .map(name => sectionForProduct(name, byCompetitor[name] || []))
    .join('\n');

  const header = [
    '# **Competitive Intelligence Report**',
    '',
    '---',
    '',
    `*Report generated via heuristic fallback (no AI model). Data from ${coverage.totalThreads} threads and ${coverage.totalComments} comments across ${coverage.subredditsUsed} subreddits over ${coverage.days} days.*`,
    '',
  ].join('\n');

  const takeaways = [
    '## **ðŸ’¡ Strategic Takeaways**',
    '',
    '1. Focus on repeatedly mentioned pain points and turn them into roadmap items.',
    '2. Emphasize strengths users already praise in your marketing and onboarding.',
    '3. Track new launches closely to adjust positioning within 1â€“2 weeks of release.',
    '',
    '---',
    `*This fallback ensures the report is always generated even if the serverless analysis fails.*`,
  ].join('\n');

  return [header, '## **Competitor Analysis**', '', competitorsMd, takeaways].join('\n');
}

function truncate(s: string, n: number): string {
  if (!s) return '';
  return s.length > n ? s.slice(0, n - 1) + 'â€¦' : s;
}

function buildCsv(items: RawItem[]): string {
  const headers = ['Competitor', 'User Feedback', 'Score', 'Subreddit', 'Thread Link'];
  const rows = items.slice(0, 200).map(i => [
    i.matchedCompetitor,
    '"' + truncate(i.title_or_text.replace(/"/g, '""'), 180) + '"',
    String(i.score ?? 0),
    `r/${i.subreddit}`,
    i.thread_url,
  ]);
  return [headers.join(','), ...rows.map(r => r.join(','))].join('\n');
}

// Stream-aware API call helper
async function streamCall(
  endpoint: string,
  data: unknown,
  onLog: LogCallback
): Promise<unknown> {
  try {
    const response = await fetch(endpoint, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(data),
    });
    
    // Read raw text first to guard against empty/HTML responses from platform timeouts
    const rawText = await response.text();
    
    if (!response.ok) {
      // Try to parse JSON error, otherwise surface raw text
      try {
        const errJson = rawText ? JSON.parse(rawText) : {};
        throw new Error(errJson.error || `Request to ${endpoint} failed (${response.status}).`);
      } catch {
        throw new Error(`Request to ${endpoint} failed (${response.status}). Body: ${rawText?.slice(0, 200) || 'empty response'}`);
      }
    }
    
    // Try to parse JSON; if invalid, surface raw body for debugging
    try {
      const parsed = rawText ? JSON.parse(rawText) : null;
      if (parsed == null) throw new Error('Empty JSON body');
      return parsed;
    } catch (e) {
      throw new Error(`Invalid JSON from ${endpoint}. Body: ${rawText?.slice(0, 200) || 'empty'}`);
    }
  } catch (error) {
    onLog(`[System] Error calling ${endpoint}: ${error instanceof Error ? error.message : 'Unknown'}`);
    throw error;
  }
}

// Legacy function for backward compatibility
export async function runAnalysis(
  input: AnalyzeInput, 
  onProgress?: ProgressCallback
): Promise<AnalysisResult> {
  const logCallback: LogCallback = (log: string) => {
    console.log(log);
  };
  
  const progressCallback: ProgressUpdateCallback = (progress: { percentage: number; taskIndex: number }) => {
    const taskNames = ['context', 'mine-me', 'mine-competitors', 'filter', 'classify', 'compose', 'csv'];
    onProgress?.(taskNames[progress.taskIndex] || 'processing', `${Math.round(progress.percentage)}% complete`);
  };
  
  return runAnalysisWithStreaming(input, logCallback, progressCallback);
}

export function copyToClipboard(text: string): Promise<void> {
  return navigator.clipboard.writeText(text);
}

export function downloadFile(content: string, filename: string, mimeType: string = 'text/plain') {
  const blob = new Blob([content], { type: mimeType });
  const url = URL.createObjectURL(blob);
  const link = document.createElement('a');
  link.href = url;
  link.download = filename;
  document.body.appendChild(link);
  link.click();
  document.body.removeChild(link);
  URL.revokeObjectURL(url);
}

export function exportMarkdown(report: ReportSections): void {
  const fullReport = [
    report.header,
    report.launches,
    report.loving,
    report.notLoving,
    report.strategicRead,
    report.takeaways,
  ].filter(Boolean).join('\n\n');
  
  const timestamp = new Date().toISOString().split('T')[0];
  downloadFile(fullReport, `competitor-analysis-${timestamp}.md`, 'text/markdown');
}

export function exportCsv(report: ReportSections): void {
  const timestamp = new Date().toISOString().split('T')[0];
  downloadFile(report.appendixCsv, `competitor-analysis-${timestamp}.csv`, 'text/csv');
}
